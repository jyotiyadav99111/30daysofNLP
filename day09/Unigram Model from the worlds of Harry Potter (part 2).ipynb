{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram Model from the worlds of Harry Potter (part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous article, we talked about the unigram model from the scratch and calculated the probabilities corresponding to every line of the Harry Potter book 1. The next step in the exercise is the evaluation of the model. Now the question comes. which type of function to be used to compare results. Let's look at the theory part of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "during the training, the probability for each word was calculated. The evaluation will be performed on the Harry Potter Book 2 and Book 3. Since these are the book's sequels, so must be having a very high probability for the entire corpus. Let's say we have two sentences for evaluation: \"I am the king of the world. Nobody can dare to stop me.\"\n",
    "\n",
    "As described in the last article, due to the independence assumption of the occurrence of the word in the text,\n",
    "\n",
    "P(word) = count(word)/Total_words\n",
    "\n",
    "Therefore, the total probability of the above text can be written as:\n",
    "\n",
    "P(I) P(am) P(the) P(king) P(of) P(the) P(world) P(.) P(Nobody) P(can) P(dare) P(to) P(stop) P(me) P(.)\n",
    "\n",
    "so, the overall probability of bothering the books can be calculated through this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Log-Likelihood as Evaluation Metric\n",
    "\n",
    "As depicted above the overall probability is the product of individual word probabilities. Therefore, if we take log on both sides it ends up as the sum of the log of word probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened to the words which are not in the training set but the test set?\n",
    "\n",
    "As those words are not available in the training set, the word's probability of occurring in the test set should be zero and log of zero = -infinity. Therefore, average log-likelihood becomes -infinity.\n",
    "\n",
    "To avoid the above problem, we use **Laplace smoothening**. This implies adding \"1\" to the number of occurrences during evaluation and the total number of new words in the denominator for each probability. So, if 20 new words have been observed in the HP book 2 then the probabilities will change to the following:\n",
    "\n",
    "P(new word) = 1/ (total_count + 20)\n",
    "\n",
    "P(existing words) = (count(word) + 1)/ (total_count + 20)\n",
    "\n",
    "the general notation for the entire formula is depicted below. In our case, we have taken s=1.\n",
    "\n",
    "This change in the evaluation will affect the probabilities not only in absolute terms but also in relative terms. Considering the example of count 10 and 20, before smoothing it's two times 10. But, post smoothing it will become 11 and 21 which leaves a multiple of ~0.47. This effect can be seen more in the smaller values but as the absolute count increases the difference becomes relatively less significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this modified version, the evaluation will be made based on the average log-likelihood metrix. The code for this complete exercise will be released tomorrow.\n",
    "\n",
    "I hope you liked the article. Stay Tuned for more!!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
