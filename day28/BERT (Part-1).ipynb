{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT (Part-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2019, Google released a breakthrough in the NLP domain. It has introduced the concept that has become the state-of-art in this domain. Google search engine primarily uses that for search optimization. BERT has been pre-trained on a huge dictionary of around 2500 million words.\n",
    "\n",
    "### Properties\n",
    "\n",
    "1. Bidirectional training: In general, the training is done on inputs to generate output. But, here the training is done from output to input as well.\n",
    "\n",
    "2. Unsupervised language representation: It is another advantage of this model. One doesn't require target values to train the model as it is pre-trained on a lot of data.\n",
    "\n",
    "3. Pre-trained using only a plain text corpus: It is trained on Wikipedia data and a book corpus which constitutes a total of 3,300 million words.\n",
    "\n",
    "### Basic Architecture\n",
    "\n",
    "It is based combined two forms of models:\n",
    "\n",
    "1. Masked LM: This is a part of text pre-processing. The 15% of the words are masked before feeding it to the algorithm. Later on, the algorithm is asked to predict these to improve the contextual model.\n",
    "\n",
    "2. Next Sentence Prediction(NSP): This is to ask the algorithm to predict if the second sentence will come after the first sentence. To do so, 50% of the text is kept with the right second sentence, and the other half is replaced with the random sentences from the corpus.\n",
    "\n",
    "3. Transformers: The entire model is based on the transformers.\n",
    "\n",
    "The encoder and decoder layers are as usual. The model has been made n a way that resembles the learning of a child in the school to learn the language. This is similar to the idea behind the neural network which is inspired by brains.\n",
    "\n",
    "The other details will be revealed in the next article. Stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
